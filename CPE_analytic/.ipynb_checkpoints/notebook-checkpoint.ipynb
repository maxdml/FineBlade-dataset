{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = ['tsung']\n",
    "clients_and_tools = ['pcap', 'tsung', 'sql_dump']\n",
    "\n",
    "EV_MAP = dict(\n",
    "    tls_reneg = 'TLS',\n",
    "    redos = 'Redos',\n",
    "    laughs = 'Laughs',\n",
    "    slowloris = 'Slowloris',\n",
    "    rudy = 'RUDY',\n",
    "    flood = 'Flash Crowd'\n",
    ")\n",
    "\n",
    "def plot_attacks(traffic, ax, x, lims, dotext, events_map):\n",
    "    for index, event in enumerate(traffic.name.unique()):\n",
    "        if event in clients_and_tools:\n",
    "            continue\n",
    "\n",
    "        event_occur = traffic[traffic.name == event]\n",
    "        \n",
    "        for i in range(0, event_occur.shape[0], 2):\n",
    "            start_row = event_occur.iloc[i]\n",
    "            end_row = event_occur.iloc[i+1]\n",
    "        \n",
    "            start = int(start_row.time)\n",
    "            end = int(end_row.time)\n",
    "        \n",
    "            ax.axvline(start, color='r', alpha=.25)                                                                                                                                                              \n",
    "            ax.axvline(end, color='r', alpha=.25)\n",
    "            ax.fill_between(x, lims[0]/10.0, lims[1]*10.0, where= (start < np.array(x)) & (np.array(x) < end),\n",
    "                            facecolor='red', alpha=.1)\n",
    "            if dotext:\n",
    "                ax.text((start + end)/2, lims[1] * 2, events_map[event], color='maroon', fontweight='bold', ha='center')\n",
    "        ax.set_ylim([lims[0], lims[1]])\n",
    "\n",
    "def tag_attacks(traffic, events_map):\n",
    "    for index, event in enumerate(traffic.name.unique()):\n",
    "        if event in clients_and_tools:\n",
    "            continue\n",
    "\n",
    "        event_occur = traffic[traffic.name == event]\n",
    "        \n",
    "        for i in range(0, event_occur.shape[0], 2):\n",
    "            start_row = event_occur.iloc[i]\n",
    "            end_row = event_occur.iloc[i+1]\n",
    "        \n",
    "            start = int(start_row.time)\n",
    "            end = int(end_row.time)\n",
    "\n",
    "            plt.text((start + end)/2, 0.5, events_map[event], color='maroon', fontweight='bold', ha='center')#, rotation=90)\n",
    "\n",
    "            \n",
    "def plot_df(df, cols, fsize=(15,30)):\n",
    "    fig, axs = plt.subplots(nrows=len(cols), ncols=1, sharex=True, figsize=fsize)\n",
    "    tag_attacks(traffic, EV_MAP)\n",
    "    for i, column in enumerate(cols):\n",
    "        axs[i].set_title(column, y=0.5, loc='right')\n",
    "        if df.columns.contains('predictions'):\n",
    "            df_correct = df[df.predictions == True]\n",
    "            df_incorrect = df[df.predictions == False]\n",
    "            axs[i].plot(df_correct.ts, df_correct[column], 'b.')\n",
    "            axs[i].plot(df_incorrect.ts, df_incorrect[column], 'r.')\n",
    "        else:\n",
    "            axs[i].plot(df.ts, df[column], '.')\n",
    "        plot_attacks(traffic, axs[i], df.ts.values, axs[i].get_ylim(), False, EV_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "If test_exps is not given, will take whatever is not in train\n",
    "'''\n",
    "def split_train_test(df, train_exps, test_exps=None):\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    for train_exp in train_exps:\n",
    "        train_df = train_df.append(df[(df.ts >= train_exp['start_time']) & (df.ts <= train_exp['end_time'])])\n",
    "        \n",
    "    if test_exps is None:\n",
    "        test_df = df.iloc[df.index.difference(train_df.index)]\n",
    "        #Remove samples with no labels\n",
    "        notrafficlocs = test_df.traffic[test_df.traffic == ''].index\n",
    "        test_df.drop(df.index[notrafficlocs], inplace=True)\n",
    "    else:\n",
    "        for test_exp in test_exps:\n",
    "            test_df = test_df.append(df[(df.ts >= test_exp['start_time']) & (df.ts <= test_exp['end_time'])])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "'''\n",
    "Helper to index experiments\n",
    "Assume all experiments are of the like: \"tsung start, event start, event ends, tsung ends\"\n",
    "returns {'tls_reneg': [{'start_time': 123, 'end_time' : 456}], 'redos' : ...}\n",
    "'''\n",
    "def sort_experiments(traffic):\n",
    "    tsung_traffic = traffic[traffic.name == 'tsung']\n",
    "    exp_indexes = tsung_traffic.index.tolist()\n",
    "    exp_tuples = list(zip(exp_indexes, exp_indexes[1:]))[::2]\n",
    "\n",
    "    experiments = {}\n",
    "    for ev in exp_tuples:\n",
    "        ev_start = ev[0]\n",
    "        ev_end = ev[1]\n",
    "        exp_label = traffic.iloc[ev_start+1]['name']\n",
    "        if exp_label in experiments:\n",
    "            experiments[exp_label].append({'start_time' : traffic.iloc[ev_start].time, 'end_time' : traffic.iloc[ev_end].time})\n",
    "        else:\n",
    "            experiments[exp_label] = [{'start_time' : traffic.iloc[ev_start].time, 'end_time' : traffic.iloc[ev_end].time}]\n",
    "\n",
    "    return experiments\n",
    "\n",
    "'''\n",
    "Squash datapoints between two experiments\n",
    "'''\n",
    "def squeeze_smore(df, traffic):\n",
    "    # get only client traffic\n",
    "    tsung_traffic = traffic[traffic.name == 'tsung'].reset_index()\n",
    "    # drop first and last rows:\n",
    "    tsung_traffic.drop(tsung_traffic.index[[0,tsung_traffic.shape[0] - 1]], inplace=True)\n",
    "\n",
    "    times = list(map(round, tsung_traffic.time.tolist()))\n",
    "    times_tuples = list(zip(times, times[1:]))[::2]\n",
    "\n",
    "    smore_df = df.copy(deep=True)\n",
    "    # Drop traffic in between experiments\n",
    "    for t in times_tuples:\n",
    "        indexes = df[(df.ts > t[0]) & (df.ts < t[1])].index\n",
    "\n",
    "        smore_df.drop(smore_df.index[indexes], inplace=True)\n",
    "\n",
    "    return smore_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(percentiles, start, end, stats=None, msu_type=None, flatten_by=['name', 'percentile', 'msu_type_id']):\n",
    "    if not isinstance(percentiles, list):\n",
    "        percentiles = [percentiles]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Get components' stats\n",
    "    if msu_type is None:\n",
    "        msu_ids = db.get_msus().msu_id.unique()\n",
    "    else:\n",
    "        msu_ids = db.get_msus(msu_type_id=msu_type).msu_id.unique()\n",
    "        \n",
    "    df_msu = db.get_multi_stat_timeseries(percentile=percentiles, start=start, end=end, round_to=1.0, stat_name=stats, msu_id=msu_ids)\n",
    "    if not (df_msu is None or df_msu.empty):\n",
    "        df_msu = db.flatten_timeseries(df_msu, flatten_by)\n",
    "        df = df_msu\n",
    "\n",
    "    #Get runtimes' stats\n",
    "    rt_ids = db.get_runtimes().id.values\n",
    "    rt_ids = [1,2,3]\n",
    "    df_rt = db.get_multi_stat_timeseries(percentile=percentiles, start=start, end=end, round_to=1.0, stat_name=stats, runtime_id=rt_ids)\n",
    "    if df_rt is None or df_rt.empty:\n",
    "        return df\n",
    "\n",
    "    df_rt.loc[:, ['name']] = df_rt.loc[:, ['name']] + '_rt'\n",
    "    df_rt = db.flatten_timeseries(df_rt, ['name', 'percentile'])\n",
    "    df = df.merge(df_rt, on='ts')\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_built_df(entry, flatten_type, percentiles, clear_cache=False, stats=None, msu_type=None):\n",
    "    if clear_cache is True:\n",
    "        del(cache[entry])\n",
    "\n",
    "    start = min(events[events.status == 'start'].time)\n",
    "    end = max(events[events.status == 'end'].time)\n",
    "\n",
    "    if entry not in cache:\n",
    "        df = build_dataframe(percentiles, start, end, flatten_by=flatten_type, stats=stats, msu_type=msu_type)\n",
    "        df = db.label_timeseries(df, traffic)\n",
    "        cache[entry] = df\n",
    "    else:\n",
    "        df = cache[entry]\n",
    "    \n",
    "    no = ['level_0', 'ts', 'index', 'STAT', 'LIFETIME'] # Convenient\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if (next((s for s in no if s in col), None)) or ('_size_' in col):\n",
    "            continue\n",
    "    #     # MinMax (0-1) scaling\n",
    "    #     df[col] = df[col].sub(df[col].min()).div((df[col].max() - df[col].min()))\n",
    "        cols.append(col)\n",
    "\n",
    "    #Remove \"holes\" in between concatenated experiments\n",
    "    return squeeze_smore(df, traffic), cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globals and config\n",
    "# Update this cell with your setup \n",
    "repo = '/your/path/to/CPE_analytic'\n",
    "cfg = dict(\n",
    "    db_name='dbname',\n",
    "    db_user='dbuser',\n",
    "    db_port='dbport',\n",
    "    db_ip='dbip',\n",
    "    db_pwd='dbpwd'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CPE_analytic.database.db_api import DbApi\n",
    "db = DbApi(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pk          time       name status  start_rate  end_rate  rate  duration                            url  sockets\n",
      "2      3    160.340000      flood  start           0         0   300       300                              0        0\n",
      "3      4    460.340000      flood    end           0         0   300       300                              0        0\n",
      "7      8    770.356731      flood  start           0         0   450       300                              0        0\n",
      "8      9   1070.356731      flood    end           0         0   450       300                              0        0\n",
      "12    15   1380.374171      flood  start           0         0   600       300                              0        0\n",
      "13    16   1680.374171      flood    end           0         0   600       300                              0        0\n",
      "15    20   1839.236419       pcap  start           0         0     0       601                              0        0\n",
      "16    21   2440.236419       pcap    end           0         0     0       601                              0        0\n",
      "19    24   1990.236419      redos  start          20        25     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "20    25   2290.236419      redos    end          20        25     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "22    27   2449.377303       pcap  start           0         0     0       601                              0        0\n",
      "23    28   3050.377303       pcap    end           0         0     0       601                              0        0\n",
      "26    31   2600.377303      redos  start          20        50     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "27    32   2900.377303      redos    end          20        50     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "29    34   3059.485160       pcap  start           0         0     0       601                              0        0\n",
      "30    35   3660.485160       pcap    end           0         0     0       601                              0        0\n",
      "33    38   3210.495160      redos  start          30        50     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "34    39   3510.495160      redos    end          30        50     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "36    41   3669.661945       pcap  start           0         0     0       601                              0        0\n",
      "37    42   4270.661945       pcap    end           0         0     0       601                              0        0\n",
      "40    45   3820.591945      redos  start          50        50     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "41    46   4120.591945      redos    end          50        50     0       300  /?regex=aaaaaaaaaaaaaaaaaaaab        0\n",
      "45    50   4430.525820       rudy  start           0         0     0       300                              0    15000\n",
      "46    51   4730.525820       rudy    end           0         0     0       300                              0    15000\n",
      "50    57   5040.874742       rudy  start           0         0     0       300                              0    17500\n",
      "51    58   5340.874742       rudy    end           0         0     0       300                              0    17500\n",
      "55    64   5650.952402       rudy  start           0         0     0       300                              0    20000\n",
      "56    65   5950.952402       rudy    end           0         0     0       300                              0    20000\n",
      "60    71   6261.090807       rudy  start           0         0     0       300                              0    24500\n",
      "61    72   6561.090807       rudy    end           0         0     0       300                              0    24500\n",
      "65    78   6871.333951  slowloris  start           0         0     0       300                              0    15000\n",
      "66    79   7171.333951  slowloris    end           0         0     0       300                              0    15000\n",
      "70    85   7481.531975  slowloris  start           0         0     0       300                              0    17500\n",
      "71    86   7781.531975  slowloris    end           0         0     0       300                              0    17500\n",
      "75    92   8091.368347  slowloris  start           0         0     0       300                              0    20000\n",
      "76    93   8391.368347  slowloris    end           0         0     0       300                              0    20000\n",
      "80    99   8700.639645  slowloris  start           0         0     0       300                              0    24500\n",
      "81   100   9000.639645  slowloris    end           0         0     0       300                              0    24500\n",
      "85   106   9310.739825  tls_reneg  start          25        25     0       300                   /?regex=fsda        0\n",
      "86   107   9610.739825  tls_reneg    end          25        25     0       300                   /?regex=fsda        0\n",
      "90   113   9921.012042  tls_reneg  start          25        25     0       300                   /?regex=fsda        0\n",
      "91   114  10221.012042  tls_reneg    end          25        25     0       300                   /?regex=fsda        0\n",
      "95   120  10531.393155  tls_reneg  start          25        50     0       300                   /?regex=fsda        0\n",
      "96   121  10831.393155  tls_reneg    end          25        50     0       300                   /?regex=fsda        0\n",
      "100  127  11141.421100  tls_reneg  start          50        50     0       300                   /?regex=fsda        0\n",
      "101  128  11441.421100  tls_reneg    end          50        50     0       300                   /?regex=fsda        0\n",
      "105  134  11751.803880     laughs  start          10        10     0       300                           /xml        0\n",
      "106  135  12051.803880     laughs    end          10        10     0       300                           /xml        0\n",
      "110  141  12362.315244     laughs  start          10        20     0       300                           /xml        0\n",
      "111  142  12662.315244     laughs    end          10        20     0       300                           /xml        0\n",
      "115  148  12972.781281     laughs  start          20        20     0       300                           /xml        0\n",
      "116  149  13272.781281     laughs    end          20        20     0       300                           /xml        0\n"
     ]
    }
   ],
   "source": [
    "events = db.get_events()\n",
    "traffic = events[~events.name.isin(clients_and_tools)]\n",
    "print(traffic.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df, cols = get_built_df('avg_all', ['name'], [100], clear_cache=False)\n",
    "plot_df(df, cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
